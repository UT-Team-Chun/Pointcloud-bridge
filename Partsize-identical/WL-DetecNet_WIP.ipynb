{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-04T16:39:21.883368Z",
     "start_time": "2024-10-04T16:39:21.425247Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from data_prep.BridgeDataLoader import LWBridgeDataset\n",
    "import torch\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import importlib\n",
    "import models.pointnet2_sem_seg_msg as MODEL\n",
    "from tool_utils.tool_utils import *\n",
    "import provider\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "ae8c9ae11acb7f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:43:39.254984Z",
     "start_time": "2024-10-04T16:43:39.252491Z"
    }
   },
   "source": [
    "rootpath=os.getcwd()\n",
    "datapath='D:\\\\Work\\\\Pointcloud-WL\\\\Pointcloud-bridge\\\\data\\\\bridge-5cls-fukushima'\n",
    "\n",
    "print(datapath)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Work\\Pointcloud-WL\\Pointcloud-bridge\\data\\bridge-5cls-fukushima\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "86d7359b2c63ca41",
   "metadata": {},
   "source": [
    "## Defining the component label"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c352b72aeb99d91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:39:30.144236Z",
     "start_time": "2024-10-04T16:39:30.141312Z"
    }
   },
   "source": [
    "classes = ['abutment', 'girder', 'deck', 'parapet', 'noise']\n",
    "class2label = {cls: i for i, cls in enumerate(classes)} # {'abutment': 0, 'girder': 1, 'deck': 2, 'parapet': 3, 'noise': 4}\n",
    "seg_classes = class2label\n",
    "seg_label_to_cat = {} # {0: 'abutment', 1: 'girder', 2: 'deck', 3: 'parapet', 4: 'noise'}\n",
    "for i, cat in enumerate(seg_classes.keys()):\n",
    "    seg_label_to_cat[i] = cat\n",
    "\n",
    "class2color = {'abutment': [229, 158, 221], 'girder':[0, 11, 195], 'deck': [173, 219, 225], 'parapet': [230, 0, 0], 'noise': [0, 169, 58]}\n",
    "label2color = {classes.index(cls): class2color[cls] for cls in classes}"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "ecc04f617836e637",
   "metadata": {},
   "source": [
    "## Defining the parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "38f2705c81cf827b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:39:31.973652Z",
     "start_time": "2024-10-04T16:39:31.969915Z"
    }
   },
   "source": [
    "NUM_CLASSES = 5  # Adjust based on your dataset\n",
    "NUM_POINT = 4096\n",
    "BATCH_SIZE = 64\n",
    "ROOT_DIR = '.'  # Adjust this to your project root directory\n",
    "# Configuration\n",
    "\n",
    "class Config:\n",
    "    model = 'pointnet2_sem_seg_msg'\n",
    "    batch_size = 64\n",
    "    epoch = 128\n",
    "    learning_rate = 0.001\n",
    "    gpu = '0'\n",
    "    optimizer = 'Adam'\n",
    "    log_dir = None\n",
    "    decay_rate = 1e-4\n",
    "    npoint = 4096\n",
    "    step_size = 10\n",
    "    lr_decay = 0.7\n",
    "\n",
    "config = Config()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "34b2a75f2e355aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:39:33.968280Z",
     "start_time": "2024-10-04T16:39:33.964426Z"
    }
   },
   "source": [
    "# Create directories\n",
    "def create_directories():\n",
    "    timestr = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    experiment_dir = Path(ROOT_DIR) / 'log' / 'sem_seg' / (config.log_dir or timestr)\n",
    "    experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoints_dir = experiment_dir / 'checkpoints'\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    log_dir = experiment_dir / 'logs'\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    return experiment_dir, checkpoints_dir, log_dir"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "48de7c883a81097b",
   "metadata": {},
   "source": [
    "## load datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "374f39f542fa9607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:39:35.650276Z",
     "start_time": "2024-10-04T16:39:35.647193Z"
    }
   },
   "source": [
    "# Load datasets\n",
    "def load_datasets(root):\n",
    "    print(\"Loading training data...\")\n",
    "    TRAIN_DATASET = LWBridgeDataset(split='train', data_root=root, num_point=NUM_POINT, \n",
    "                                    block_size=1.0, sample_rate=1.0, num_class=NUM_CLASSES)\n",
    "    print(\"Loading test data...\")\n",
    "    TEST_DATASET = LWBridgeDataset(split='test', data_root=root, num_point=NUM_POINT, \n",
    "                                   block_size=1.0, sample_rate=1.0, num_class=NUM_CLASSES)\n",
    "    \n",
    "    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, \n",
    "                                                  shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, \n",
    "                                                 shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    return TRAIN_DATASET, TEST_DATASET, trainDataLoader, testDataLoader"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "ca3549454208b63b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:39:37.363356Z",
     "start_time": "2024-10-04T16:39:37.359507Z"
    }
   },
   "source": [
    "# Initialize models\n",
    "def initialize_model(num_classes, experiment_dir):\n",
    "    classifier = MODEL.get_model(num_classes).cuda()\n",
    "    criterion = MODEL.get_loss().cuda()\n",
    "    \n",
    "    try:\n",
    "        # 使用 weights_only=True 和 map_location\n",
    "        checkpoint = torch.load(\n",
    "            str(experiment_dir / 'checkpoints' / 'best_model.pth'),\n",
    "            weights_only=True,\n",
    "            map_location=torch.device('cuda')  # 或使用 'cuda' 如果你在 GPU 上运行\n",
    "        )\n",
    "\n",
    "        # 如果你只加载权重，你可能需要单独处理 epoch 信息\n",
    "        start_epoch = 0  # 或者从配置文件中读取\n",
    "        classifier.load_state_dict(checkpoint)\n",
    "        print('Use pretrained model')\n",
    "    except Exception as e:\n",
    "        print(f'Error loading model: {e}')\n",
    "        print('Starting training from scratch...')\n",
    "        start_epoch = 0\n",
    "        classifier.apply(weights_init)\n",
    "\n",
    "    return classifier, criterion, start_epoch\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0.0)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "d29de87565f60260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:39:38.583457Z",
     "start_time": "2024-10-04T16:39:38.579535Z"
    }
   },
   "source": [
    "# Training function\n",
    "def train(classifier, criterion, optimizer, trainDataLoader, logger):\n",
    "    classifier.train()\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for points, target in tqdm(trainDataLoader, total=len(trainDataLoader), smoothing=0.9):\n",
    "        # for my loss function, strcture-oriented loss (SOL)\n",
    "        points_raw = points.float().cuda() # output.shape: [16, 4096, 9]\n",
    "        target_SOL = target.long().cuda() # output.shape: [16, 4096]\n",
    "        \n",
    "        points = points.data.numpy() # points.shape: [16, 4096, 9]\n",
    "        points[: , : , : 3] = provider.rotate_point_cloud_z(points[: , : , : 3])\n",
    "        points = torch.Tensor(points)\n",
    "        points, target = points.float().cuda(), target.long().cuda()\n",
    "        \n",
    "        # adjust the shape of 'points' to suit the input_size of 'classifier'\n",
    "        points = points.transpose(2, 1) # output.shape: [16, 9, 4096]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        seg_pred, trans_feat = classifier(points)\n",
    "        # for my loss function, strcture-oriented loss (SOL)\n",
    "        seg_pred_SOL = seg_pred # output.shape: [16, 4096, NUM_CLASSES]\n",
    "        \n",
    "        seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)  # output.shape: ([16 * 4096, NUM_CLASSES])\n",
    "        # target.view(-1, 1): [16 * 4096, 1], target.view(-1, 1)[ : , 0]: [16 * 4096]\n",
    "        target = target.view(-1, 1)[:, 0]\n",
    "        \n",
    "        loss = criterion(seg_pred_SOL, target_SOL, points_raw, seg_pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred_choice = seg_pred.cpu().data.max(1)[1].numpy()\n",
    "        correct = np.sum(pred_choice == target.cpu().data.numpy())\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE * NUM_POINT)\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(trainDataLoader), total_correct / float(total_seen)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "7487b84347cd8feb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:39:39.347230Z",
     "start_time": "2024-10-04T16:39:39.342156Z"
    }
   },
   "source": [
    "# Evaluation function\n",
    "def evaluate(classifier, criterion, testDataLoader, num_classes):\n",
    "    classifier.eval()\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(num_classes)]\n",
    "    total_correct_class = [0 for _ in range(num_classes)]\n",
    "    total_iou_deno_class = [0 for _ in range(num_classes)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for points, target in tqdm(testDataLoader, total=len(testDataLoader), smoothing=0.9):\n",
    "            # for my loss function, strcture-oriented loss (SOL)\n",
    "            points_raw = points.float().cuda() # output.shape: [16, 4096, 9]\n",
    "            target_SOL = target.long().cuda() # output.shape: [16, 4096]\n",
    "            \n",
    "            points = points.data.numpy()\n",
    "            points = torch.Tensor(points)\n",
    "            points, target = points.float().cuda(), target.long().cuda()\n",
    "            points = points.transpose(2, 1) # output.shape: [16, 9, 4096]\n",
    "            classifier = classifier.eval()\n",
    "            # seg_pred --> segmentation prediction, shape: [BATCH_SIZE, NUM_POINT, NUM_CLASSES] ([16, 4096, NUM_CLASSES])\n",
    "            seg_pred, trans_feat = classifier(points)\n",
    "\n",
    "            # for my loss function, strcture-oriented loss (SOL)\n",
    "            seg_pred_SOL = seg_pred # output.shape: [16, 4096, NUM_CLASSES]\n",
    "\n",
    "            pred_val = seg_pred.contiguous().cpu().data.numpy() # pred_val.shape: [16, 4096, NUM_CLASSES]\n",
    "            seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES) # output.shape: [16 * 4096, NUM_CLASSES]\n",
    "            batch_label = target.cpu().data.numpy() # batch_label.shape: [16, 4096]\n",
    "            target = target.view(-1, 1)[: , 0] # output.shape: [16 * 4096]\n",
    "            \n",
    "            loss = criterion(seg_pred_SOL, target_SOL, points_raw, seg_pred, target)\n",
    "            loss_sum += loss.item()\n",
    "            pred_val = np.argmax(pred_val, 2)\n",
    "            correct = np.sum((pred_val == batch_label))\n",
    "            total_correct += correct\n",
    "            total_seen += (BATCH_SIZE * NUM_POINT)\n",
    "            \n",
    "            for l in range(num_classes):\n",
    "                total_seen_class[l] += np.sum((batch_label == l))\n",
    "                total_correct_class[l] += np.sum((pred_val == l) & (batch_label == l))\n",
    "                total_iou_deno_class[l] += np.sum(((pred_val == l) | (batch_label == l)))\n",
    "    \n",
    "    mIoU = np.mean(np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=np.float32) + 1e-6))\n",
    "    return loss_sum / len(testDataLoader), total_correct / float(total_seen), mIoU"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "243ebf4dcc175e59",
   "metadata": {},
   "source": [
    "# Mian Structure"
   ]
  },
  {
   "cell_type": "code",
   "id": "860d4e5a8617746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:39:43.235161Z",
     "start_time": "2024-10-04T16:39:43.230055Z"
    }
   },
   "source": [
    "log_dir_TB = './log/Tensorboard'\n",
    "writer = SummaryWriter(log_dir_TB)\n",
    "# tensorboard --logdir=Partsize-identical/log/Tensorboard\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "b9fad2f9d4ebd18e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:39:43.864962Z",
     "start_time": "2024-10-04T16:39:43.863256Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e571bf7abf490e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:45:59.381128Z",
     "start_time": "2024-10-04T16:43:46.083002Z"
    }
   },
   "source": [
    "experiment_dir, checkpoints_dir, log_dir = create_directories()\n",
    "logger = setup_logging(log_dir, config.model)\n",
    "logger.info(\"Starting the training process...\")\n",
    "\n",
    "#set Tensorboard\n",
    "writer = SummaryWriter(log_dir_TB)\n",
    "\n",
    "# Set GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "\n",
    "# Load datasets\n",
    "TRAIN_DATASET, TEST_DATASET, trainDataLoader, testDataLoader = load_datasets(datapath)\n",
    "\n",
    "logger.info(f\"Number of training data: {len(TRAIN_DATASET)}\")\n",
    "logger.info(f\"Number of test data: {len(TEST_DATASET)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "labelweights of train: [0.12851092 0.29946876 0.19900215 0.0422972  0.33072102]\n",
      "Totally 48120 samples in train set.\n",
      "Loading test data...\n",
      "labelweights of test: [0.08364363 0.28175014 0.09126764 0.03026254 0.51307607]\n",
      "Totally 1428 samples in test set.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "91283b8db16923bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:46:00.296519Z",
     "start_time": "2024-10-04T16:45:59.387645Z"
    }
   },
   "source": [
    "# Initialize model\n",
    "classifier, criterion, start_epoch = initialize_model(NUM_CLASSES, experiment_dir)\n",
    "# Optimizer\n",
    "if config.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        classifier.parameters(), \n",
    "        lr=config.learning_rate, \n",
    "        betas=(0.9, 0.999), \n",
    "        eps=1e-08, \n",
    "        weight_decay=config.decay_rate\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(classifier.parameters(), lr=config.learning_rate, momentum=0.9)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "best_iou = 0"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: [Errno 2] No such file or directory: 'log\\\\sem_seg\\\\2024-10-05_01-43\\\\checkpoints\\\\best_model.pth'\n",
      "Starting training from scratch...\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "75232f89a832f6e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:46:00.306178Z",
     "start_time": "2024-10-04T16:46:00.302466Z"
    }
   },
   "source": [
    "# 打印模型结构\n",
    "print(classifier)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_model(\n",
      "  (sa1): PointNetSetAbstractionMsg(\n",
      "    (conv_blocks): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Conv2d(12, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Conv2d(12, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (bn_blocks): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0-1): 2 x BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0-1): 2 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sa2): PointNetSetAbstractionMsg(\n",
      "    (conv_blocks): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Conv2d(99, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Conv2d(99, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (bn_blocks): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sa3): PointNetSetAbstractionMsg(\n",
      "    (conv_blocks): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): Conv2d(259, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(128, 196, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Conv2d(196, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (bn_blocks): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm2d(196, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sa4): PointNetSetAbstractionMsg(\n",
      "    (conv_blocks): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Conv2d(515, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Conv2d(515, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (bn_blocks): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0-1): 2 x BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fp4): PointNetFeaturePropagation(\n",
      "    (mlp_convs): ModuleList(\n",
      "      (0): Conv1d(1536, 256, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (mlp_bns): ModuleList(\n",
      "      (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (fp3): PointNetFeaturePropagation(\n",
      "    (mlp_convs): ModuleList(\n",
      "      (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (mlp_bns): ModuleList(\n",
      "      (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (fp2): PointNetFeaturePropagation(\n",
      "    (mlp_convs): ModuleList(\n",
      "      (0): Conv1d(352, 256, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (mlp_bns): ModuleList(\n",
      "      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (fp1): PointNetFeaturePropagation(\n",
      "    (mlp_convs): ModuleList(\n",
      "      (0-2): 3 x Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (mlp_bns): ModuleList(\n",
      "      (0-2): 3 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (conv2): Conv1d(128, 5, kernel_size=(1,), stride=(1,))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33f4db19002b1ff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T12:11:02.224438Z",
     "start_time": "2024-10-03T12:11:01.379926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 16, 1024]             208\n",
      "       BatchNorm2d-2         [-1, 16, 16, 1024]              32\n",
      "            Conv2d-3         [-1, 16, 16, 1024]             272\n",
      "       BatchNorm2d-4         [-1, 16, 16, 1024]              32\n",
      "            Conv2d-5         [-1, 32, 16, 1024]             544\n",
      "       BatchNorm2d-6         [-1, 32, 16, 1024]              64\n",
      "            Conv2d-7         [-1, 32, 32, 1024]             416\n",
      "       BatchNorm2d-8         [-1, 32, 32, 1024]              64\n",
      "            Conv2d-9         [-1, 32, 32, 1024]           1,056\n",
      "      BatchNorm2d-10         [-1, 32, 32, 1024]              64\n",
      "           Conv2d-11         [-1, 64, 32, 1024]           2,112\n",
      "      BatchNorm2d-12         [-1, 64, 32, 1024]             128\n",
      "PointNetSetAbstractionMsg-13  [[-1, 3, 1024], [-1, 96, 1024]]               0\n",
      "           Conv2d-14          [-1, 64, 16, 256]           6,400\n",
      "      BatchNorm2d-15          [-1, 64, 16, 256]             128\n",
      "           Conv2d-16          [-1, 64, 16, 256]           4,160\n",
      "      BatchNorm2d-17          [-1, 64, 16, 256]             128\n",
      "           Conv2d-18         [-1, 128, 16, 256]           8,320\n",
      "      BatchNorm2d-19         [-1, 128, 16, 256]             256\n",
      "           Conv2d-20          [-1, 64, 32, 256]           6,400\n",
      "      BatchNorm2d-21          [-1, 64, 32, 256]             128\n",
      "           Conv2d-22          [-1, 96, 32, 256]           6,240\n",
      "      BatchNorm2d-23          [-1, 96, 32, 256]             192\n",
      "           Conv2d-24         [-1, 128, 32, 256]          12,416\n",
      "      BatchNorm2d-25         [-1, 128, 32, 256]             256\n",
      "PointNetSetAbstractionMsg-26  [[-1, 3, 256], [-1, 256, 256]]               0\n",
      "           Conv2d-27          [-1, 128, 16, 64]          33,280\n",
      "      BatchNorm2d-28          [-1, 128, 16, 64]             256\n",
      "           Conv2d-29          [-1, 196, 16, 64]          25,284\n",
      "      BatchNorm2d-30          [-1, 196, 16, 64]             392\n",
      "           Conv2d-31          [-1, 256, 16, 64]          50,432\n",
      "      BatchNorm2d-32          [-1, 256, 16, 64]             512\n",
      "           Conv2d-33          [-1, 128, 32, 64]          33,280\n",
      "      BatchNorm2d-34          [-1, 128, 32, 64]             256\n",
      "           Conv2d-35          [-1, 196, 32, 64]          25,284\n",
      "      BatchNorm2d-36          [-1, 196, 32, 64]             392\n",
      "           Conv2d-37          [-1, 256, 32, 64]          50,432\n",
      "      BatchNorm2d-38          [-1, 256, 32, 64]             512\n",
      "PointNetSetAbstractionMsg-39  [[-1, 3, 64], [-1, 512, 64]]               0\n",
      "           Conv2d-40          [-1, 256, 16, 16]         132,096\n",
      "      BatchNorm2d-41          [-1, 256, 16, 16]             512\n",
      "           Conv2d-42          [-1, 256, 16, 16]          65,792\n",
      "      BatchNorm2d-43          [-1, 256, 16, 16]             512\n",
      "           Conv2d-44          [-1, 512, 16, 16]         131,584\n",
      "      BatchNorm2d-45          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-46          [-1, 256, 32, 16]         132,096\n",
      "      BatchNorm2d-47          [-1, 256, 32, 16]             512\n",
      "           Conv2d-48          [-1, 384, 32, 16]          98,688\n",
      "      BatchNorm2d-49          [-1, 384, 32, 16]             768\n",
      "           Conv2d-50          [-1, 512, 32, 16]         197,120\n",
      "      BatchNorm2d-51          [-1, 512, 32, 16]           1,024\n",
      "PointNetSetAbstractionMsg-52  [[-1, 3, 16], [-1, 1024, 16]]               0\n",
      "           Conv1d-53              [-1, 256, 64]         393,472\n",
      "      BatchNorm1d-54              [-1, 256, 64]             512\n",
      "           Conv1d-55              [-1, 256, 64]          65,792\n",
      "      BatchNorm1d-56              [-1, 256, 64]             512\n",
      "PointNetFeaturePropagation-57              [-1, 256, 64]               0\n",
      "           Conv1d-58             [-1, 256, 256]         131,328\n",
      "      BatchNorm1d-59             [-1, 256, 256]             512\n",
      "           Conv1d-60             [-1, 256, 256]          65,792\n",
      "      BatchNorm1d-61             [-1, 256, 256]             512\n",
      "PointNetFeaturePropagation-62             [-1, 256, 256]               0\n",
      "           Conv1d-63            [-1, 256, 1024]          90,368\n",
      "      BatchNorm1d-64            [-1, 256, 1024]             512\n",
      "           Conv1d-65            [-1, 128, 1024]          32,896\n",
      "      BatchNorm1d-66            [-1, 128, 1024]             256\n",
      "PointNetFeaturePropagation-67            [-1, 128, 1024]               0\n",
      "           Conv1d-68            [-1, 128, 4096]          16,512\n",
      "      BatchNorm1d-69            [-1, 128, 4096]             256\n",
      "           Conv1d-70            [-1, 128, 4096]          16,512\n",
      "      BatchNorm1d-71            [-1, 128, 4096]             256\n",
      "           Conv1d-72            [-1, 128, 4096]          16,512\n",
      "      BatchNorm1d-73            [-1, 128, 4096]             256\n",
      "PointNetFeaturePropagation-74            [-1, 128, 4096]               0\n",
      "           Conv1d-75            [-1, 128, 4096]          16,512\n",
      "      BatchNorm1d-76            [-1, 128, 4096]             256\n",
      "          Dropout-77            [-1, 128, 4096]               0\n",
      "           Conv1d-78              [-1, 5, 4096]             645\n",
      "================================================================\n",
      "Total params: 1,882,237\n",
      "Trainable params: 1,882,237\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 2519.53\n",
      "Params size (MB): 7.18\n",
      "Estimated Total Size (MB): 2526.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(classifier, input_size=(9, 4096)) "
   ]
  },
  {
   "cell_type": "code",
   "id": "b4fcd0981701b5dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T18:21:23.388792Z",
     "start_time": "2024-10-04T16:46:00.321769Z"
    }
   },
   "source": [
    "total_epochs = config.epoch\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, config.epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    logger.info(f'Epoch {epoch+1}/{config.epoch}')\n",
    "    \n",
    "    lr = max(config.learning_rate * (config.lr_decay ** (epoch // config.step_size)), 1e-5)\n",
    "    logger.info(f'Learning rate: {lr}')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    #train\n",
    "    train_loss, train_acc = train(classifier, criterion, optimizer, trainDataLoader, logger)\n",
    "    logger.info(f'Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "    \n",
    "    # 记录训练指标\n",
    "    writer.add_scalar('Train/Loss', train_loss, epoch)\n",
    "    writer.add_scalar('Train/Accuracy', train_acc, epoch)\n",
    "    writer.add_scalar('Train/LearningRate', lr, epoch)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, str(checkpoints_dir / 'model.pth'))\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_loss, eval_acc, mIoU = evaluate(classifier, criterion, testDataLoader, NUM_CLASSES)\n",
    "    logger.info(f'Eval - Loss: {eval_loss:.4f}, Accuracy: {eval_acc:.4f}, mIoU: {mIoU:.4f}')\n",
    "    \n",
    "    # calculate each epochs time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step(eval_loss)  # 对于 ReduceLROnPlateau，传入验证损失\n",
    "\n",
    "   \n",
    "    # 记录评估指标\n",
    "    writer.add_scalar('Eval/Loss', eval_loss, epoch)\n",
    "    writer.add_scalar('Eval/Accuracy', eval_acc, epoch)\n",
    "    writer.add_scalar('Eval/mIoU', mIoU, epoch)\n",
    "    \n",
    "    if mIoU >= best_iou:\n",
    "        best_iou = mIoU\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'mIoU': mIoU,\n",
    "        }, str(checkpoints_dir / 'best_model.pth'))\n",
    "    \n",
    "    logger.info(f'Best mIoU: {best_iou:.4f}')\n",
    "    \n",
    "# 关闭 SummaryWriter\n",
    "writer.close()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 751/751 [11:52<00:00,  1.05it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.03it/s]\n",
      "100%|██████████| 751/751 [12:15<00:00,  1.02it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.05it/s]\n",
      "100%|██████████| 751/751 [12:20<00:00,  1.01it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.04it/s]\n",
      "100%|██████████| 751/751 [12:24<00:00,  1.01it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.06it/s]\n",
      "100%|██████████| 751/751 [12:24<00:00,  1.01it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.04it/s]\n",
      "100%|██████████| 751/751 [12:21<00:00,  1.01it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.04it/s]\n",
      "100%|██████████| 751/751 [12:19<00:00,  1.02it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.06it/s]\n",
      "100%|██████████| 751/751 [12:22<00:00,  1.01it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.06it/s]\n",
      "100%|██████████| 751/751 [11:57<00:00,  1.05it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:38<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:37<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:30<00:00,  1.09it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:38<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:38<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:30<00:00,  1.09it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:35<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.09it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:38<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:34<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.10it/s]\n",
      "100%|██████████| 751/751 [11:35<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:32<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.09it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.10it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:39<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.09it/s]\n",
      "100%|██████████| 751/751 [11:35<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.10it/s]\n",
      "100%|██████████| 751/751 [11:32<00:00,  1.09it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.16it/s]\n",
      "100%|██████████| 751/751 [11:33<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.10it/s]\n",
      "100%|██████████| 751/751 [11:32<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:35<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:31<00:00,  1.09it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:34<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:34<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.09it/s]\n",
      "100%|██████████| 751/751 [11:39<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:35<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:32<00:00,  1.09it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.15it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.08it/s]\n",
      "100%|██████████| 751/751 [11:42<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:40<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.08it/s]\n",
      "100%|██████████| 751/751 [11:40<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:41<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:34<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.13it/s]\n",
      "100%|██████████| 751/751 [11:41<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:37<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:34<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:33<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:38<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.08it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.13it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.10it/s]\n",
      "100%|██████████| 751/751 [11:41<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.13it/s]\n",
      "100%|██████████| 751/751 [11:38<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.09it/s]\n",
      "100%|██████████| 751/751 [11:40<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:37<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:40<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:42<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.13it/s]\n",
      "100%|██████████| 751/751 [11:38<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.13it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.07it/s]\n",
      "100%|██████████| 751/751 [11:47<00:00,  1.06it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.09it/s]\n",
      "100%|██████████| 751/751 [11:48<00:00,  1.06it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.10it/s]\n",
      "100%|██████████| 751/751 [11:48<00:00,  1.06it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.02it/s]\n",
      "100%|██████████| 751/751 [12:21<00:00,  1.01it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.02it/s]\n",
      "100%|██████████| 751/751 [12:16<00:00,  1.02it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.15it/s]\n",
      "100%|██████████| 751/751 [12:13<00:00,  1.02it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.03it/s]\n",
      "100%|██████████| 751/751 [11:41<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:26<00:00,  1.09it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.04it/s]\n",
      "100%|██████████| 751/751 [12:22<00:00,  1.01it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.04it/s]\n",
      "100%|██████████| 751/751 [12:01<00:00,  1.04it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.04it/s]\n",
      "100%|██████████| 751/751 [12:23<00:00,  1.01it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.04it/s]\n",
      "100%|██████████| 751/751 [11:52<00:00,  1.05it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:21<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n",
      "100%|██████████| 751/751 [11:15<00:00,  1.11it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n",
      "100%|██████████| 751/751 [11:32<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.16it/s]\n",
      "100%|██████████| 751/751 [11:24<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.08it/s]\n",
      "100%|██████████| 751/751 [11:27<00:00,  1.09it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.16it/s]\n",
      "100%|██████████| 751/751 [11:07<00:00,  1.12it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.20it/s]\n",
      "100%|██████████| 751/751 [11:25<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.17it/s]\n",
      "100%|██████████| 751/751 [11:21<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.10it/s]\n",
      "100%|██████████| 751/751 [11:24<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.13it/s]\n",
      "100%|██████████| 751/751 [11:22<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.10it/s]\n",
      "100%|██████████| 751/751 [11:12<00:00,  1.12it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.09it/s]\n",
      "100%|██████████| 751/751 [11:19<00:00,  1.11it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.13it/s]\n",
      "100%|██████████| 751/751 [11:23<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:24<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.15it/s]\n",
      "100%|██████████| 751/751 [11:21<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.08it/s]\n",
      "100%|██████████| 751/751 [11:15<00:00,  1.11it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.15it/s]\n",
      "100%|██████████| 751/751 [11:19<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.19it/s]\n",
      "100%|██████████| 751/751 [11:09<00:00,  1.12it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:21<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.15it/s]\n",
      "100%|██████████| 751/751 [11:21<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:53<00:00,  1.05it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.04it/s]\n",
      "100%|██████████| 751/751 [11:59<00:00,  1.04it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.16it/s]\n",
      "100%|██████████| 751/751 [11:20<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.21it/s]\n",
      "100%|██████████| 751/751 [11:13<00:00,  1.11it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:23<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:29<00:00,  1.09it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.15it/s]\n",
      "100%|██████████| 751/751 [11:42<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.05it/s]\n",
      "100%|██████████| 751/751 [11:36<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:34<00:00,  1.08it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:42<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.05it/s]\n",
      "100%|██████████| 751/751 [12:11<00:00,  1.03it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.03it/s]\n",
      "100%|██████████| 751/751 [12:19<00:00,  1.02it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.13it/s]\n",
      "100%|██████████| 751/751 [11:23<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:22<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.15it/s]\n",
      "100%|██████████| 751/751 [11:40<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.01it/s]\n",
      "100%|██████████| 751/751 [12:19<00:00,  1.02it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.17it/s]\n",
      "100%|██████████| 751/751 [11:27<00:00,  1.09it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.15it/s]\n",
      "100%|██████████| 751/751 [11:22<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.15it/s]\n",
      "100%|██████████| 751/751 [11:21<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.11it/s]\n",
      "100%|██████████| 751/751 [11:20<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.16it/s]\n",
      "100%|██████████| 751/751 [11:20<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:42<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.09it/s]\n",
      "100%|██████████| 751/751 [11:17<00:00,  1.11it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.13it/s]\n",
      "100%|██████████| 751/751 [11:16<00:00,  1.11it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:38<00:00,  1.07it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.16it/s]\n",
      "100%|██████████| 751/751 [11:16<00:00,  1.11it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:20<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:45<00:00,  1.06it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.02it/s]\n",
      "100%|██████████| 751/751 [12:27<00:00,  1.01it/s]\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.17it/s]\n",
      "100%|██████████| 751/751 [12:12<00:00,  1.03it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.04it/s]\n",
      "100%|██████████| 751/751 [12:15<00:00,  1.02it/s]\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.05it/s]\n",
      "100%|██████████| 751/751 [12:28<00:00,  1.00it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.03it/s]\n",
      "100%|██████████| 751/751 [11:22<00:00,  1.10it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.14it/s]\n",
      "100%|██████████| 751/751 [11:18<00:00,  1.11it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n",
      "100%|██████████| 751/751 [11:26<00:00,  1.09it/s]\n",
      "100%|██████████| 22/22 [00:19<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "150955c7fcfce9ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:56:37.287897Z",
     "start_time": "2024-10-02T08:56:37.281587Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    experiment_dir, checkpoints_dir, log_dir = create_directories()\n",
    "    logger = setup_logging(log_dir, config.model)\n",
    "    logger.info(\"Starting the training process...\")\n",
    "    \n",
    "    # Set GPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "    \n",
    "    # Load datasets\n",
    "    TRAIN_DATASET, TEST_DATASET, trainDataLoader, testDataLoader = load_datasets(datapath)\n",
    "    \n",
    "    logger.info(f\"Number of training data: {len(TRAIN_DATASET)}\")\n",
    "    logger.info(f\"Number of test data: {len(TEST_DATASET)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    classifier, criterion, start_epoch = initialize_model(NUM_CLASSES, experiment_dir)\n",
    "    \n",
    "    # Optimizer\n",
    "    if config.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            classifier.parameters(), \n",
    "            lr=config.learning_rate, \n",
    "            betas=(0.9, 0.999), \n",
    "            eps=1e-08, \n",
    "            weight_decay=config.decay_rate\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(classifier.parameters(), lr=config.learning_rate, momentum=0.9)\n",
    "    \n",
    "    best_iou = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, config.epoch):\n",
    "        logger.info(f'Epoch {epoch+1}/{config.epoch}')\n",
    "        \n",
    "        lr = max(config.learning_rate * (config.lr_decay ** (epoch // config.step_size)), 1e-5)\n",
    "        logger.info(f'Learning rate: {lr}')\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        train_loss, train_acc = train(classifier, criterion, optimizer, trainDataLoader, logger)\n",
    "        logger.info(f'Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, str(checkpoints_dir / 'model.pth'))\n",
    "        \n",
    "        eval_loss, eval_acc, mIoU = evaluate(classifier, criterion, testDataLoader, NUM_CLASSES)\n",
    "        logger.info(f'Eval - Loss: {eval_loss:.4f}, Accuracy: {eval_acc:.4f}, mIoU: {mIoU:.4f}')\n",
    "        \n",
    "        if mIoU >= best_iou:\n",
    "            best_iou = mIoU\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'mIoU': mIoU,\n",
    "            }, str(checkpoints_dir / 'best_model.pth'))\n",
    "        \n",
    "        logger.info(f'Best mIoU: {best_iou:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4abbfe087e0024b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:59:26.220221Z",
     "start_time": "2024-10-02T08:56:38.356412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "labelweights of train: [0.12851092 0.29946876 0.19900215 0.0422972  0.33072102]\n",
      "Totally 48120 samples in train set.\n",
      "Loading test data...\n",
      "labelweights of test: [0.08364363 0.28175014 0.09126764 0.03026254 0.51307607]\n",
      "Totally 1428 samples in test set.\n",
      "No existing models, starting training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_849329/1901400961.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(str(experiment_dir / 'checkpoints/best_model.pth'))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'weights_init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 7\u001B[0m, in \u001B[0;36minitialize_model\u001B[0;34m(num_classes, experiment_dir)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m----> 7\u001B[0m     checkpoint \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mexperiment_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcheckpoints/best_model.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     start_epoch \u001B[38;5;241m=\u001B[39m checkpoint[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/work/Pointcloud-bridge/venv/lib/python3.12/site-packages/torch/serialization.py:1065\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1063\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m-> 1065\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m   1066\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m   1067\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m   1068\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m   1069\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n",
      "File \u001B[0;32m~/work/Pointcloud-bridge/venv/lib/python3.12/site-packages/torch/serialization.py:468\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 468\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/work/Pointcloud-bridge/venv/lib/python3.12/site-packages/torch/serialization.py:449\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 449\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'log/sem_seg/2024-10-02_17-56/checkpoints/best_model.pth'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[46], line 16\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of test data: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(TEST_DATASET)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Initialize model\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m classifier, criterion, start_epoch \u001B[38;5;241m=\u001B[39m \u001B[43minitialize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNUM_CLASSES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexperiment_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Optimizer\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdam\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "Cell \u001B[0;32mIn[14], line 14\u001B[0m, in \u001B[0;36minitialize_model\u001B[0;34m(num_classes, experiment_dir)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNo existing models, starting training from scratch...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     13\u001B[0m     start_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 14\u001B[0m     classifier\u001B[38;5;241m.\u001B[39mapply(\u001B[43mweights_init\u001B[49m)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m classifier, criterion, start_epoch\n",
      "\u001B[0;31mNameError\u001B[0m: name 'weights_init' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f089d29b0442b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
